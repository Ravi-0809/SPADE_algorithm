\section{SPADE}

\subsection{Algorithm}
The SPADE algorithm is an apriori based sequential mining technique that makes use of the vertical data format to optimize the number of scans through sequence dataset. In this data format, each element is represented as shown in Fig. \ref{vertical} and the array of sequence ID and event ID pairs is known as the ID list of that event. These ID lists carry the information necessary to find the support of the candidates(events) and also make it easy to convert to the horizontal data representation on-the-fly with less overhead. Moreover, as the length of the frequent sequence increases, the size of its ID list decreases, resulting in very fast joins.

The SPADE algorithm is roughly implemented in 3 steps:
\begin{enumerate}
    \item Find frequent items or frequent 1-sequences
    \item Find frequent 2-sequences
    \item Find frequent n-sequences recursively
\end{enumerate}

The implementation of these three steps are discussed in detail in the next section.

\subsection{Implementation}

Due to the large dataset size and also the large sequence lengths, the algorithm takes too long to run on the complete dataset (more than 5 hours). Instead, the algorithm is implemented on a 20\% subset of the data generated using random sampling which will approximately give the same results.
The implemented algorithm has been uploaded to github \cite{b3} and was tested on the google colab platform.

\subsubsection{Computing frequent 1-sequences}

The frequent 1-sequences are proposed to be calculated using a single scan through the vertical data format. The counts of all the unique sequences that each event occurs in are stored in a hash map and compared against the min\_sup(minimum support). This is the proposed implementation given by the SPADE paper \cite{b1} but in this project code, the frequent items are found using basic pandas operations - groupby function is used to get the id list of each event and then finding the support of unique sequence ids in each id list. If the support was greater than the min\_sup, then the event, along with it's id list, was stored as a frequent item and rest of the data was discarded.

\subsubsection{Computing frequent 2-sequences}

A naive implementation of finding the frequent 2-sequences by doing an id list join of frequent 1-sequences is very inefficient due to the number of joins needed. To counter this, the original paper proposes two methods - use a preprocessing step to gather the counts of all 2-sequences above a user specified lower bound, or  Perform a vertical-to-horizontal transformation on-the-fly and computing frequent 2-sequences from the recovered horizontal database is straight-forward. In this project, the second proposed method of converting to horizontal format on-the-fly is used. The frequent 2-sequences are then eliminated by comparing the support of each frequent sequence with the min\_sup.

\subsubsection{Computing frequent n-sequences recursively}

The final step involves recursively computing frequent sequences of all lengths greater than 2 until no more frequent sequences can be found. Frequent sequences of size n are generated by joining the id lists of two n-1 length sequences. The generation of frequent sequences is seen as a graph search problem using either BFS or DFS. During the search, although the algorithm supports pruning, the paper claims that pruning did not improve the running time and also required considerable overhead and storage to prune effectively. For these reasons, pruning was not implemented in this project.

At each step, a \textit{temporal join} is performed on two n-1 length sequences to output a sequence of length n. A temporal join between two sequences can normally lead to 3 type of outputs. But since the in the given dataset, each event consists of only a single element, a temporal join between two sequences can only have one outcome.
The implementation of the temporal join was sped up multifold by using pandas techniques and tricks. The naive implementation given by other public libraries took nearly 5 hours for the this third step but the pandas implementation took only 2 minutes on the same sample of the dataset, leading to nearly 150x speedup from the traditional implementation.

After the temporal join of each n-1 length sequence with each other, the support of all the newly generated sequences is evaluated and the sequence is added to the list of frequent sequences if it's support is greater than min\_sup. The function is then recursively called using the newly generated frequent n-sequences until no more frequent sequences are generated.


Once all the three steps are complete, frequent sequences of all lengths are obtained by combining the results of all three steps.